{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.571332Z",
     "start_time": "2025-02-10T23:40:27.566876Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.615960Z",
     "start_time": "2025-02-10T23:40:27.612170Z"
    }
   },
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.667599Z",
     "start_time": "2025-02-10T23:40:27.664744Z"
    }
   },
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.715283Z",
     "start_time": "2025-02-10T23:40:27.712179Z"
    }
   },
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "9c033522-d2fc-41ac-8e3c-5e35872bf88d",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.762705Z",
     "start_time": "2025-02-10T23:40:27.759740Z"
    }
   },
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.816340Z",
     "start_time": "2025-02-10T23:40:27.808501Z"
    }
   },
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.892579Z",
     "start_time": "2025-02-10T23:40:27.859488Z"
    }
   },
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.909956Z",
     "start_time": "2025-02-10T23:40:27.907778Z"
    }
   },
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:40:27.954606Z",
     "start_time": "2025-02-10T23:40:27.951744Z"
    }
   },
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 64,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:41:04.267046Z",
     "start_time": "2025-02-10T23:41:04.234243Z"
    }
   },
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_GeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[27], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m abot\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mstream({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages}, thread):\n\u001B[1;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m event\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[1;32m      3\u001B[0m         \u001B[38;5;28mprint\u001B[39m(v[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1668\u001B[0m, in \u001B[0;36mPregel.stream\u001B[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001B[0m\n\u001B[1;32m   1664\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m stream_modes:\n\u001B[1;32m   1665\u001B[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m c: stream\u001B[38;5;241m.\u001B[39mput(\n\u001B[1;32m   1666\u001B[0m         ((), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom\u001B[39m\u001B[38;5;124m\"\u001B[39m, c)\n\u001B[1;32m   1667\u001B[0m     )\n\u001B[0;32m-> 1668\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mSyncPregelLoop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStreamProtocol\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_modes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpointer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpointer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1675\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1676\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_channels_asis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1678\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterrupt_before_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1679\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterrupt_after_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1680\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1681\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1682\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m loop:\n\u001B[1;32m   1683\u001B[0m     \u001B[38;5;66;03m# create runner\u001B[39;00m\n\u001B[1;32m   1684\u001B[0m     runner \u001B[38;5;241m=\u001B[39m PregelRunner(\n\u001B[1;32m   1685\u001B[0m         submit\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39msubmit,\n\u001B[1;32m   1686\u001B[0m         put_writes\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39mput_writes,\n\u001B[1;32m   1687\u001B[0m         schedule_task\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39maccept_push,\n\u001B[1;32m   1688\u001B[0m         node_finished\u001B[38;5;241m=\u001B[39mconfig[CONF]\u001B[38;5;241m.\u001B[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001B[1;32m   1689\u001B[0m     )\n\u001B[1;32m   1690\u001B[0m     \u001B[38;5;66;03m# enable subgraph streaming\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langgraph/pregel/loop.py:862\u001B[0m, in \u001B[0;36mSyncPregelLoop.__init__\u001B[0;34m(self, input, stream, config, store, checkpointer, nodes, specs, manager, interrupt_after, interrupt_before, output_keys, stream_keys, debug)\u001B[0m\n\u001B[1;32m    860\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack \u001B[38;5;241m=\u001B[39m ExitStack()\n\u001B[1;32m    861\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m checkpointer:\n\u001B[0;32m--> 862\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointer_get_next_version \u001B[38;5;241m=\u001B[39m \u001B[43mcheckpointer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_next_version\u001B[49m\n\u001B[1;32m    863\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointer_put_writes \u001B[38;5;241m=\u001B[39m checkpointer\u001B[38;5;241m.\u001B[39mput_writes\n\u001B[1;32m    864\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointer_put_writes_accepts_task_path \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    865\u001B[0m         signature(checkpointer\u001B[38;5;241m.\u001B[39mput_writes)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtask_path\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    866\u001B[0m         \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    867\u001B[0m     )\n",
      "\u001B[0;31mAttributeError\u001B[0m: '_GeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace59a36-3941-459e-b9d1-ac5a4a1ed3ae",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:41:11.364396Z",
     "start_time": "2025-02-10T23:41:11.355679Z"
    }
   },
   "source": [
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 200,
    "ExecuteTime": {
     "end_time": "2025-02-10T23:41:13.583572Z",
     "start_time": "2025-02-10T23:41:13.278059Z"
    }
   },
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_AsyncGeneratorContextManager' object has no attribute 'get_next_version'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m messages \u001B[38;5;241m=\u001B[39m [HumanMessage(content\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhat is the weather in SF?\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n\u001B[1;32m      2\u001B[0m thread \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfigurable\u001B[39m\u001B[38;5;124m\"\u001B[39m: {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthread_id\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m4\u001B[39m\u001B[38;5;124m\"\u001B[39m}}\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m abot\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mastream_events({\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m: messages}, thread, version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv1\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m      4\u001B[0m     kind \u001B[38;5;241m=\u001B[39m event[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m kind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_chat_model_stream\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langchain_core/runnables/base.py:1381\u001B[0m, in \u001B[0;36mRunnable.astream_events\u001B[0;34m(self, input, config, version, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001B[0m\n\u001B[1;32m   1378\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(msg)\n\u001B[1;32m   1380\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m aclosing(event_stream):\n\u001B[0;32m-> 1381\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m event \u001B[38;5;129;01min\u001B[39;00m event_stream:\n\u001B[1;32m   1382\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m event\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langchain_core/tracers/event_stream.py:781\u001B[0m, in \u001B[0;36m_astream_events_implementation_v1\u001B[0;34m(runnable, input, config, include_names, include_types, include_tags, exclude_names, exclude_types, exclude_tags, **kwargs)\u001B[0m\n\u001B[1;32m    777\u001B[0m root_name \u001B[38;5;241m=\u001B[39m config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m, runnable\u001B[38;5;241m.\u001B[39mget_name())\n\u001B[1;32m    779\u001B[0m \u001B[38;5;66;03m# Ignoring mypy complaint about too many different union combinations\u001B[39;00m\n\u001B[1;32m    780\u001B[0m \u001B[38;5;66;03m# This arises because many of the argument types are unions\u001B[39;00m\n\u001B[0;32m--> 781\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m log \u001B[38;5;129;01min\u001B[39;00m _astream_log_implementation(  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    782\u001B[0m     runnable,\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28minput\u001B[39m,\n\u001B[1;32m    784\u001B[0m     config\u001B[38;5;241m=\u001B[39mconfig,\n\u001B[1;32m    785\u001B[0m     stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m    786\u001B[0m     diff\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    787\u001B[0m     with_streamed_output_list\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    788\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    789\u001B[0m ):\n\u001B[1;32m    790\u001B[0m     run_log \u001B[38;5;241m=\u001B[39m run_log \u001B[38;5;241m+\u001B[39m log\n\u001B[1;32m    792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m encountered_start_event:\n\u001B[1;32m    793\u001B[0m         \u001B[38;5;66;03m# Yield the start event for the root runnable.\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:675\u001B[0m, in \u001B[0;36m_astream_log_implementation\u001B[0;34m(runnable, input, config, stream, diff, with_streamed_output_list, **kwargs)\u001B[0m\n\u001B[1;32m    672\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    673\u001B[0m     \u001B[38;5;66;03m# Wait for the runnable to finish, if not cancelled (eg. by break)\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39msuppress(asyncio\u001B[38;5;241m.\u001B[39mCancelledError):\n\u001B[0;32m--> 675\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m task\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langchain_core/tracers/log_stream.py:629\u001B[0m, in \u001B[0;36m_astream_log_implementation.<locals>.consume_astream\u001B[0;34m()\u001B[0m\n\u001B[1;32m    626\u001B[0m prev_final_output: Optional[Output] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    627\u001B[0m final_output: Optional[Output] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 629\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m runnable\u001B[38;5;241m.\u001B[39mastream(\u001B[38;5;28minput\u001B[39m, config, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    630\u001B[0m     prev_final_output \u001B[38;5;241m=\u001B[39m final_output\n\u001B[1;32m    631\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m final_output \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langgraph/pregel/__init__.py:1960\u001B[0m, in \u001B[0;36mPregel.astream\u001B[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001B[0m\n\u001B[1;32m   1954\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m stream_modes:\n\u001B[1;32m   1955\u001B[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1956\u001B[0m         \u001B[38;5;28;01mlambda\u001B[39;00m c: aioloop\u001B[38;5;241m.\u001B[39mcall_soon_threadsafe(\n\u001B[1;32m   1957\u001B[0m             stream\u001B[38;5;241m.\u001B[39mput_nowait, ((), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom\u001B[39m\u001B[38;5;124m\"\u001B[39m, c)\n\u001B[1;32m   1958\u001B[0m         )\n\u001B[1;32m   1959\u001B[0m     )\n\u001B[0;32m-> 1960\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mAsyncPregelLoop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1961\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1962\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStreamProtocol\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mput_nowait\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_modes\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1963\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1964\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1965\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcheckpointer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcheckpointer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1966\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnodes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnodes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1967\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspecs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1968\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1969\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream_channels_asis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1970\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterrupt_before\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterrupt_before_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1971\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterrupt_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterrupt_after_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1972\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmanager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1973\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdebug\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdebug\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1974\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m loop:\n\u001B[1;32m   1975\u001B[0m     \u001B[38;5;66;03m# create runner\u001B[39;00m\n\u001B[1;32m   1976\u001B[0m     runner \u001B[38;5;241m=\u001B[39m PregelRunner(\n\u001B[1;32m   1977\u001B[0m         submit\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39msubmit,\n\u001B[1;32m   1978\u001B[0m         put_writes\u001B[38;5;241m=\u001B[39mloop\u001B[38;5;241m.\u001B[39mput_writes,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1981\u001B[0m         node_finished\u001B[38;5;241m=\u001B[39mconfig[CONF]\u001B[38;5;241m.\u001B[39mget(CONFIG_KEY_NODE_FINISHED),\n\u001B[1;32m   1982\u001B[0m     )\n\u001B[1;32m   1983\u001B[0m     \u001B[38;5;66;03m# enable subgraph streaming\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/AI_Agent_LangGraph/venv/lib/python3.10/site-packages/langgraph/pregel/loop.py:997\u001B[0m, in \u001B[0;36mAsyncPregelLoop.__init__\u001B[0;34m(self, input, stream, config, store, checkpointer, nodes, specs, interrupt_after, interrupt_before, manager, output_keys, stream_keys, debug)\u001B[0m\n\u001B[1;32m    995\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstack \u001B[38;5;241m=\u001B[39m AsyncExitStack()\n\u001B[1;32m    996\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m checkpointer:\n\u001B[0;32m--> 997\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointer_get_next_version \u001B[38;5;241m=\u001B[39m \u001B[43mcheckpointer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_next_version\u001B[49m\n\u001B[1;32m    998\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointer_put_writes \u001B[38;5;241m=\u001B[39m checkpointer\u001B[38;5;241m.\u001B[39maput_writes\n\u001B[1;32m    999\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheckpointer_put_writes_accepts_task_path \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1000\u001B[0m         signature(checkpointer\u001B[38;5;241m.\u001B[39maput_writes)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtask_path\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1001\u001B[0m         \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m     )\n",
      "\u001B[0;31mAttributeError\u001B[0m: '_AsyncGeneratorContextManager' object has no attribute 'get_next_version'"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
